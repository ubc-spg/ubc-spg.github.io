<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>N Christin | UBC Security &amp; Privacy Group</title>
    <link>https://spg.cs.ubc.ca/author/n-christin/</link>
      <atom:link href="https://spg.cs.ubc.ca/author/n-christin/index.xml" rel="self" type="application/rss+xml" />
    <description>N Christin</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 14 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://spg.cs.ubc.ca/media/icon_hu08a0612ece3264b0c60a561067d923a4_34621_512x512_fill_lanczos_center_3.png</url>
      <title>N Christin</title>
      <link>https://spg.cs.ubc.ca/author/n-christin/</link>
    </image>
    
    <item>
      <title>GFWeb: Measuring the Great Firewall&#39;s Web Censorship at Scale</title>
      <link>https://spg.cs.ubc.ca/publication/2024_usenixsec_gfweb/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://spg.cs.ubc.ca/publication/2024_usenixsec_gfweb/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Censorship systems such as the Great Firewall (GFW) have been continuously
refined to enhance their filtering capabilities. However, most prior
studies, and in particular the GFW, have been limited in scope and
conducted over short time periods, leading to gaps in our understanding of
the GFW&amp;rsquo;s evolving Web censorship mechanisms over time. We introduce GFWeb,
a novel system designed to discover domain blocklists used by the GFW for
censoring Web access. GFWeb exploits GFW&amp;rsquo;s bidirectional and loss-tolerant
blocking behavior to enable testing hundreds of millions of domains on a
monthly basis, thereby facilitating large-scale longitudinal measurement of
HTTP and HTTPS blocking mechanisms.&lt;/p&gt;
&lt;p&gt;Over the course of 20 months, GFWeb has tested a total of 1.02 billion
domains, and detected 943K and 55K pay-level domains censored by the GFW&amp;rsquo;s
HTTP and HTTPS filters, respectively. To the best of our knowledge, our
study represents the most extensive set of domains censored by the GFW ever
discovered to date, many of which have never been detected by prior
systems. Analyzing the longitudinal dataset collected by GFWeb, we observe
that the GFW has been upgraded to mitigate several issues previously
identified by the research community, including overblocking and failure in
reassembling fragmented packets. More importantly, we discover that the
GFW&amp;rsquo;s bidirectional blocking is not symmetric as previously thought, i.e.,
it can only be triggered by certain domains when probed from inside the
country. We discuss the implications of our work on existing censorship
measurement and circumvention efforts. We hope insights gained from our
study can help inform future research, especially in monitoring censorship
and developing new evasion tools.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Generation of Web Censorship Probe Lists</title>
      <link>https://spg.cs.ubc.ca/publication/2024_popets/</link>
      <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://spg.cs.ubc.ca/publication/2024_popets/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Domain probe lists&amp;mdash;used to determine which URLs to probe for Web
censorship&amp;mdash;play a critical role in Internet censorship measurement
studies. Indeed, the size and accuracy of the domain probe list limits the
set of censored pages that can be detected; inaccurate lists can lead to an
incomplete view of the censorship landscape or biased results. Previous
efforts to generate domain probe lists have been mostly manual or
crowdsourced. This approach is time-consuming, prone to errors, and does
not scale well to the ever-changing censorship landscape.&lt;/p&gt;
&lt;p&gt;In this paper, we explore methods for automatically generating probe lists
that are both comprehensive and up-to-date for Web censorship measurement.
We start from an initial set of 139,957 unique URLs from various existing
test lists consisting of pages from a variety of languages to generate new
candidate pages. By analyzing content from these URLs (i.e., performing
topic and keyword extraction), expanding these topics, and using them as a
feed to search engines, our method produces 119,255 new URLs across 35,147
domains. We then test the new candidate pages by attempting to access each
URL from servers in eleven different global locations over a span of four
months to check for their connectivity and potential signs of censorship.
Our measurements reveal that our method discovered over 1,400 domains&amp;mdash;not
present in the original dataset&amp;mdash;we suspect to be blocked. In short,
automatically updating probe lists is possible, and can help further
automate censorship measurements at scale.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
